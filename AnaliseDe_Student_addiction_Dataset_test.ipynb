{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW3mlDs5FpHlBSVlcsBJEL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eddieHerman-lab/Analise_DE_Diamantes/blob/main/AnaliseDe_Student_addiction_Dataset_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise de Dados sobre Vícios em Estudantes\n",
        "\n",
        "Introdução\n",
        "Este projeto visa analisar os padrões de comportamento relacionados a vícios em estudantes utilizando técnicas de clustering, análise de componentes principais (PCA) e regressão linear. O objetivo é identificar grupos distintos de estudantes com base em seus comportamentos e características."
      ],
      "metadata": {
        "id": "5EajfQ5EYmOI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g96Hq7PQ0na8",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Importar bibiotecas necessarias"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparacao de dados,tratamento e limpeza\n",
        "Descrição dos Dados O dataset contém informações sobre comportamentos relacionados a vícios em estudantes, incluindo experimentação, desempenho acadêmico, isolamento social, problemas financeiros e de saúde mental. Dados exportados do Kaggle."
      ],
      "metadata": {
        "id": "ybcpdiNQY3mU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/student_addiction_dataset_test.csv') #Exportando os dados\n",
        "# Visualizar as primeiras linhas do DataFrame\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Mapear 'NÃO' para False e 'SIM' para True em todas as colunas\n",
        "df = df.replace({'No': False, 'Yes': True})\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "gZHEqu5yXDuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "2723c632-bfd4-4584-9f58-6c34d08b2c0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/student_addiction_dataset_test.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1b04d64d69b9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/student_addiction_dataset_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Exportando os dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Visualizar as primeiras linhas do DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Mapear 'NÃO' para False e 'SIM' para True em todas as colunas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'No'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Yes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/student_addiction_dataset_test.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analise exploratoria dos dados;\n",
        "Estatisticas descritivas e visualizacoes iniciais;\n",
        "Descricao de medias por grupos;\n",
        "Descricao de desvio padrao por grupos;\n",
        "Realizacao de tabela cruzada entre as colunas Experimentation e Addiction Class"
      ],
      "metadata": {
        "id": "9KDnxEU-ZOMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby('Addiction_Class')\n",
        "mean_values = grouped.mean()\n",
        "std_values = grouped.std()\n",
        "print(mean_values, 'Mean values for each group')\n",
        "print(std_values, 'Standard deviation for each group')\n",
        "\n",
        "differences = []\n",
        "for column in df.columns:\n",
        "    counts = df[column].value_counts()\n",
        "    Sim_counts = counts.get(True, 0)\n",
        "    Nao_counts = counts.get(False, 0)\n",
        "    diff = Sim_counts - Nao_counts\n",
        "    differences.append((column, diff))\n",
        "\n",
        "cross_tab = pd.crosstab(df['Experimentation'], df['Addiction_Class'])\n",
        "print(cross_tab, 'Cross tabs Experimentation and Addiction')\n",
        "print(cross_tab.corr())"
      ],
      "metadata": {
        "id": "O0sOaWRmYUbw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "84af6f14-8fed-47ad-952a-3fe657e6f4bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-1ed0d50656cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Addiction_Class'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmean_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstd_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Mean values for each group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Standard deviation for each group'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizacao dos dados\n"
      ],
      "metadata": {
        "id": "yvIAy9VGa-fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizando os dados\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "rqJ8ppvBbr-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise de Clustering com K-Means e PCA\n",
        "MODELAGEM\n",
        "\n",
        "### Por que se ultiliza K-Means?\n",
        "O algoritmo de K-Means é uma técnica de aprendizado não supervisionado usada para particionar um conjunto de dados em um número pré-definido de clusters. Cada ponto de dado pertence ao cluster cujo centro está mais próximo, resultando em uma divisão dos dados em grupos distintos. Esta abordagem é útil quando queremos identificar subgrupos em nossos dados que possuem características similares, o que pode fornecer insights valiosos sobre padrões de comportamento ou características comuns entre diferentes grupos.\n",
        "\n",
        "### Processo de Clustering\n",
        "1. Normalização dos Dados:\n",
        "   Antes de aplicar o K-Means, foi normalizado  os dados para garantir que todas as variáveis estejam na mesma escala. Isso é feito utilizando `StandardScaler`, que transforma os dados para que tenham média zero e desvio padrão um."
      ],
      "metadata": {
        "id": "M7dJf252cADw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrando o melhor número de clusters com o método do cotovelo\n",
        "sse = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(df_scaled)\n",
        "    sse.append(kmeans.inertia_)"
      ],
      "metadata": {
        "id": "hIjcAcbFbuqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicacao com Matplotlib para melhor entendimento\n"
      ],
      "metadata": {
        "id": "OTmdZe0FcNXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, sse, marker='o')\n",
        "plt.xlabel('Númber of Clusters')\n",
        "plt.ylabel('SSE (Sum of square errors)')\n",
        "plt.title('Elbow method')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lMcac_KYcGJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicação do K-Means:\n",
        "Após determinar o número apropriado de clusters , aplicamos o K-Means para particionar os dados em clusters."
      ],
      "metadata": {
        "id": "pSd1595SejL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A partir do gráfico, escolha o número apropriado de clusters (por exemplo, 3)\n",
        "num_clusters = 3\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(df_scaled)\n",
        "df['Cluster'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "cFZ4M-XicSaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análise de Componentes Principais (PCA)\n",
        "A PCA é uma técnica de redução de dimensionalidade que transforma um conjunto de variáveis correlacionadas em um conjunto menor de variáveis não correlacionadas chamadas componentes principais. Esta abordagem é útil para visualizar dados de alta dimensão em gráficos de duas ou três dimensões, mantendo a maior parte da variância original dos dados.\n",
        "\n",
        "Aplicação da PCA:\n",
        "Aplicamos a PCA para reduzir os dados a cinco componentes principais, o que facilita a visualização e a análise dos clusters."
      ],
      "metadata": {
        "id": "wRipOKZLe6dF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA com mais componentes\n",
        "pca = PCA(n_components=5)  # Aumentando o número de componentes para 5\n",
        "principalComponents = pca.fit_transform(df_scaled)\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(\"Explained variance by component:\")\n",
        "print(explained_variance)\n",
        "num_columns = df.shape[1]\n",
        "# DataFrame dos componentes principais\n",
        "principal_df = pd.DataFrame(data=principalComponents, columns=[f'PC{i+1}' for i in range(5)])\n",
        "print(\"Principal components:\")\n",
        "components = pd.DataFrame(pca.components_, columns=df.columns[:-1], index=[f'PC{i+1}' for i in range(5)])\n",
        "print(components)"
      ],
      "metadata": {
        "id": "1vGS3DEheeLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizacao dos clusters"
      ],
      "metadata": {
        "id": "tHMFuZOxgMH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar os clusters nas primeiras duas componentes principais\n",
        "plt.scatter(principalComponents[:, 0], principalComponents[:, 1], c=df['Cluster'])\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Clusters visualized on first two principal components')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HC2EbebjgHAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajustando o Modelo de K-Means com Mais Componentes Principais\n",
        "Após a normalização dos dados e a aplicação do K-Means inicialmente, foi utilizado a Análise de Componentes Principais (PCA) para reduzir a dimensionalidade dos dados. A PCA transforma os dados originais em componentes principais que explicam a maior parte da variação nos dados. Usar mais componentes principais significa capturar mais variabilidade dos dados, o que pode levar a uma melhor separação entre os clusters.\n",
        "\n",
        "Isso permite que o modelo K-Means trabalhe com uma representação mais rica dos dados, que pode melhorar a qualidade do clustering.\n",
        "\n",
        "Motivação\n",
        "\n",
        "A principal motivação para ajustar o K-Means com mais componentes principais é melhorar a qualidade do clustering. Componentes principais adicionais podem capturar mais variância nos dados, o que pode ajudar o K-Means a identificar padrões mais claros e formar clusters mais coesos. Além disso, a utilização de componentes principais reduzidos pode facilitar a visualização e interpretação dos clusters.Como um melhor vizualizacao,e grupos mais distintos com menor dimensionalidade  e possivel inferir fatores de causalidade relacionados a eles\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgAHGKsjjR2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar o modelo de KMeans com mais componentes principais\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(principalComponents)"
      ],
      "metadata": {
        "id": "_EySdA61hBIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretação das Distribuições\n",
        "A partir dessas visualizações, você pode observar como cada variável se comporta dentro de cada cluster. Por exemplo:\n",
        "\n",
        "Experimentation: Pode-se observar que o nível de experimentação varia significativamente entre os clusters, indicando que alguns grupos de estudantes são mais propensos a experimentar substâncias do que outros.\n",
        "Academic Performance Decline: A queda no desempenho acadêmico pode ser mais pronunciada em alguns clusters, sugerindo uma correlação entre certos tipos de vícios e o impacto negativo no desempenho acadêmico.\n",
        "Social Isolation: A análise pode revelar que determinados clusters têm uma maior incidência de isolamento social, o que pode estar associado a comportamentos de vício."
      ],
      "metadata": {
        "id": "IvhMDD4YCEkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analisando os clusters\n",
        "colunas = ['Experimentation', 'Academic_Performance_Decline', 'Social_Isolation', 'Financial_Issues', 'Physical_Mental_Health_Problems']\n",
        "for coluna in colunas:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(data=df, x=coluna, hue='Cluster', palette='viridis')\n",
        "    plt.title(f'Distribuição de {coluna} por Cluster')\n",
        "    plt.xlabel(coluna)\n",
        "    plt.ylabel('Contagem')\n",
        "    plt.legend(title='Cluster')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "iYOlKRi_jao-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Silhouette Score\n",
        "O Silhouette Score é uma métrica usada para avaliar a qualidade do clustering. Ele mede o quão semelhante um ponto é ao seu próprio cluster em comparação com outros clusters."
      ],
      "metadata": {
        "id": "wybtbm3F-9fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Silhouette Score com mais componentes principais\n",
        "score = silhouette_score(principalComponents, df['Cluster'])\n",
        "print(f'Silhouette Score with more components: {score}')"
      ],
      "metadata": {
        "id": "YKv27ESetDKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regressão Linear com Componentes Principais e Validação Cruzada\n",
        "Objetivo\n",
        "O objetivo desta parte da análise é ajustar um modelo de regressão linear para prever a classe de vício (Addiction_Class) com base nas componentes principais obtidas a partir da PCA. Além disso, usamos validação cruzada (cross-validation) para avaliar a performance do modelo."
      ],
      "metadata": {
        "id": "e6e3g4xqDPig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regressão Linear com mais componentes principais\n",
        "#Preparação dos Dados para Regressão:\n",
        "#X são as componentes principais obtidas da PCA.\n",
        "#y é a variável alvo, Addiction_Class.\n",
        "model = LinearRegression()\n",
        "X = principalComponents\n",
        "y = df['Addiction_Class']\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "ByVZEHOwCYQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validação Cruzada (Cross-Validation):\n",
        "Utilizamos a técnica de validação cruzada com 5 folds (cv=5) para avaliar a performance do modelo de regressão linear.\n",
        "A validação cruzada divide os dados em 5 partes. Em cada iteração, uma parte é usada como conjunto de teste e as outras quatro como conjunto de treinamento. Este processo é repetido 5 vezes, com cada parte sendo usada uma vez como conjunto de teste.\n",
        "cross_val_score é utilizado para calcular as métricas de avaliação (neste caso, o coeficiente de determinação\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        " ) para cada iteração."
      ],
      "metadata": {
        "id": "xnaRtC_sDiTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cross Validation com mais componentes principais\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "print(f'Cross-validation scores with more components: {scores}')\n",
        "print(f'Average cross-validation score with more components: {np.mean(scores)}')"
      ],
      "metadata": {
        "id": "DIuKGWb-De8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupando por cluster e calculando a média para cada característica\n",
        "cluster_means = df.groupby('Cluster').mean()\n",
        "print(cluster_means)"
      ],
      "metadata": {
        "id": "1tQv0pY_FXwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlação entre Variáveis para cada Cluster e Visualizações\n",
        "Objetivo\n",
        "O objetivo dessa parte da análise é entender a relação entre diferentes variáveis dentro de cada cluster. A correlação entre variáveis pode fornecer insights sobre como os fatores se relacionam entre si dentro dos clusters identificados pelo KMeans.\n",
        "\n",
        "Passos Detalhados\n",
        "Agrupamento por Cluster:\n",
        "\n",
        "Os dados são agrupados por cluster para realizar a análise de correlação dentro de cada grupo.\n",
        "Matriz de Correlação:\n",
        "\n",
        "A matriz de correlação é calculada para cada cluster usando o método corr() do pandas. Esta matriz mostra a correlação entre cada par de variáveis.\n",
        "Uma correlação alta (perto de 1 ou -1) indica uma forte relação linear entre duas variáveis, enquanto uma correlação baixa (perto de 0) indica pouca ou nenhuma relação linear.\n",
        "Visualização com Heatmap:\n",
        "\n",
        "Usamos sns.heatmap do Seaborn para visualizar a matriz de correlação. O heatmap é uma representação gráfica onde cores mais escuras indicam correlações mais fortes (positivas ou negativas)."
      ],
      "metadata": {
        "id": "TLJd2tNNHBPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada cluster, calcular as correlações entre as variáveis\n",
        "for cluster in df['Cluster'].unique():\n",
        "    df_cluster = df[df['Cluster'] == cluster]\n",
        "    correlation_matrix = df_cluster.corr()\n",
        "    print(correlation_matrix)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(f'Heatmap de Correlação - Cluster {cluster}')\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "znaWCnCIFetx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusões\n",
        "Predominância de Não Experimentação (0): Em todos os clusters, a maioria dos estudantes não experimentou a substância (0). Isso pode sugerir que a experimentação não é o comportamento dominante na maioria dos estudantes analisados.\n",
        "Cluster 2 Tem a Maior População de Não Experimentadores: O cluster 2 destaca-se por ter o maior número de estudantes que não experimentaram, indicando que esse cluster pode estar mais associado a comportamentos mais conservadores ou menos propensos à experimentação.\n",
        "Menor Variação Entre Clusters Para Experimentação (1): Embora todos os clusters tenham uma quantidade menor de estudantes que experimentaram (1), as proporções parecem relativamente similares, sugerindo que a experimentação é um comportamento menos comum e, portanto, menos discriminativo entre os clusters."
      ],
      "metadata": {
        "id": "I_01rheXJtaT"
      }
    }
  ]
}